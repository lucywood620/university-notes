\documentclass{article}[18pt]
\usepackage{../../../../format}
\lhead{Algorithms and Data Structures}


\begin{document}
\begin{center}
\underline{\huge Hash Tables}
\end{center}
\section{Key-Value Pairs}
Suppose we want to store data consisting of pairs of \textbf{keys} and \textbf{values}. For example:
\begin{center}
\begin{tabular}{c|c}
name&number\\
\hline
Smith&123512\\
Jones&174322\\
Jackson&192852
\end{tabular}
\end{center}
We want to be able to \textbf{look up} the values using the keys, and
\begin{itemize}
\item The total amount of data might be very large
\item Our algorithms might perform many look ups
\end{itemize}
More
\begin{itemize}
\item It would be easier to find if the list was sorted, this would mean that a guess could be made, then use binary search to get closer to the correct data. This requires log(N) operations, rather than N, as you would with an unsorted list
\item A good way to optimise this would be to take the first 3 letters of each name, and convert to the 3 numbers at the start of the name, then insert the data at that point. This would make finding the data easy as the conversion could just be done again
\end{itemize}
\section{Hash Tables}
\begin{itemize}
\item A \textbf{hash table} consists of a bucket array and a hash function
\item A \textbf{bucket array} for a hash table is an array A of size N, where each cell of A is thought of as a \textbf{bucket} storing a collection of key-value pairs
\item The size N of the array is called the \textbf{capacity} of the hash table
\item A \textbf{hash function} h is a function mapping each key k to an integer in the range [0,n-1], where N is the capacity of the hash table (To make an address for each key)
\item The main idea is to use h(k) as an index into the bucket array A. That is, we store the key-value pair (k,v) in the bucket A[h(k)]
\item If there are two keys with the same hash value h(k), then two different entries will be mapped to the same bucket in A. In this case, way say that a collision has occurred
\end{itemize}
Questions
\begin{itemize}
\item Can there be entries in the hash table with the \textbf{same key}? - No (Entries would be indistinguishable)
\item Can there be entries in the hash table with the \textbf{same value}? - Yes 
\item Can there be entries in the hash table with the \textbf{same key and same value}?
\end{itemize}
How a hash function works:
\begin{itemize}
\item A hash function is usually specified as the composition of two functions:
\begin{itemize}
\item hash code: keys to integers
\item compression function: integers to [0,N-1] 
\end{itemize}
\item The hash code is applied first, and the compression function is applied next onto the result
\end{itemize}
The goal of the hash function is to \textbf{disperse} the keys in an apparently random way
\section{Hash functions}
Ideal goal: Scramble the keys \textbf{uniformly}\\
\\
More practically:
\begin{itemize}
\item Hash function should be \textbf{efficiently computable}
\item Each table position \textbf{equally likely} for each key
\end{itemize}
Some compression functions:
\begin{itemize}
\item \textbf{division}: take integer mod N
\item \textbf{multiply add and divide (MAD)} y maps to ay+B mod N. Where a and b are nonnegative integers
\end{itemize}
\subsection{Collisions}
\begin{itemize}
\item There are several ways to deal with \textbf{collisions}. Whichever method we choose to deal with them, a large number of collisions reduces the performance of the hash table
\item A good hash function minimises the collisions as much as possible
\end{itemize}
We will discuss four different methods for handling collisions:
\begin{itemize}
\item Separate chaining
\item Linear probing
\item Quadratic probing
\item Double hashing
\end{itemize}
\subsection{Separate Chaining}
Each bucket A[i] stores a list holding the entries (k,v) such that h(k)=i\\
\\
Separate chaining performance:
\begin{itemize}
\item Cost is proportional to length of list
\item Average length = N/M (N is amount of data, M is size of array)
\item Worst case: all keys hash to the same list
\end{itemize}
More
\begin{itemize}
\item Apply hash function (mod length) to each key
\item Whenever two things go in the same place in the array, create a linked list inside the bucket
\end{itemize}
\subsection{Linear Probing}
The other three methods, called open addressing schemes,
store at most one entry to each bucket.
\begin{itemize}
\item In linear probing, if we try to insert an entry (k, v) into a
bucket A[i] that is already occupied, where i = h(k), then
we try next at A[(i + 1) mod N].
\item If A[(i + 1) mod N] is also occupied, then we try A[(i + 2)
mod N], and so on, until we find an empty bucket that can
accept the new entry.
\item The name linear probing comes from the fact that
accessing a cell of the bucket array can be viewed as a
probe.
\end{itemize}
Description of method:
\begin{itemize}
\item When you have a collision, just go to the next available place (Mod N, so loop round)
\item In long lists, clusters of data are formed
\end{itemize}

Costs:
\begin{itemize}
\item Insert and search cost depend on length of cluster.
\item Average length of cluster is N/M.
\item Worst case: all keys hash to same cluster
\end{itemize}
\subsection{Quadratic Probing}
\begin{itemize}
\item Quadratic probing iteratively tries the buckets
A[(i + f(j)) mod N], for j = 0, 1, 2, . . . , where f(j) = $j^2$
until finding an empty bucket.
\item Quadratic probing avoids clustering patterns that occur
with linear probing. However, it creates its own kind of
clustering, called secondary clustering.
\item The quadratic probing strategy may not find an empty
bucket in A even if one exists.
\end{itemize}
Method:
\begin{itemize}
\item Trying i,i+1,i+9...
\item Other than that it is the same as linear probing
\item The motivation for this is to jump quickly out of clusters
\item However this creates clusters in squares
\end{itemize}
\subsection{Double Hashing}
\begin{itemize}
\item In this approach, we choose a secondary hash function, h', and if h maps some key k to a bucket A[i], with i = h(k), that is already occupied, then we iteratively try the buckets
A[(i + f(j)) mod N], for j = 0, 1, 2, . . . , where f(j) = jh'(k)
\item A common choice of secondary hash function is 
$$h'(k) = q - (k \ mod \ q)$$
for some prime number $q < N$.
\item The secondary hash function should not evaluate to zero.
\end{itemize}
Method:
\begin{itemize}
\item First try i, then i+h'(k) then i+2$\times$h'(k)
\end{itemize}
\subsection{Comparison}
\begin{itemize}
\item The open addressing schemes are more memory efficient
compared to separate chaining.
\item Regarding running times, in experimental and theoretical
analyses, the separate chaining method is either
competitive or faster than the other methods.
\item If memory space is not a major issue, the
collision-handling method of choice is separate chaining.
\end{itemize}
\subsection{Deletions}
\begin{itemize}
\item \textbf{Deletions} from the hash table must not hinder future
searches. If a bucket is simply left empty this will hinder
future probes.
\item But the bucket should not be left unusable.
\item To solve these problems we use \textbf{tombstones}: a marker that
is left in a bucket after a deletion.
\item If a tombstone is encountered when searching along a
probe sequence to find a key, we know to continue.
\item If a tombstone is encountered during insertion, then we
should continue probing (to avoid creating \textbf{duplicates}), but
then the new record can placed in the bucket where the
tombstone was found.
\item The use of tombstones lengthens the average probe
sequence distance.
\item Two possible remedies:
\begin{itemize}
\item Local reorganization: after deleting a key, continue to follow
the probe sequence of that key and move records into the
vacated bucket. (This will not work for all collision resolution
policies).
\item Periodically rehash the table by reinserting all records into a
new hash table. And if you have a record of which keys are
accessed most these can be placed where they will be
found most easily.
\end{itemize}

\end{itemize}
This is describing a dictionary in Python

\end{document}