\documentclass{article}[18pt]
\input{../../../../format}
\lhead{Software Methodologies - Machine Learning}


\begin{document}
\begin{center}
\underline{\huge Odds and Logistic Regression}
\end{center}
\section{Odds}
\begin{definition}[Odds]
	A numerical expression, expressed as a pair of numbers
\end{definition}
\begin{important}[Odds vs Probabilities]
Odds are not probabilities, odds is the number of successes vs number of failures, whereas probabilities is number of successes against total number of cases
\end{important}
$$\dfrac{\text{Probabilitiy(success)}}{\text{Probability(failure)}}=\text{Odds}$$
This is the same as
$$\dfrac{\text{Probabilitiy(success)}}{1-\text{Probability(success)}}=\text{Odds}$$
Convention is that $p$ represents the probability of success and $q$ represents the probability of failure
\subsection{Log of odds}
Where the number of successes $>$ number of failures:
\begin{itemize}
	\item Odds against success is between 0 and 1
	\item Odds in favour of success is between 1 and $+\infty$
\end{itemize}
This makes the odds against success look way smaller, so take the logs to make everything symmetrical\\
\\
Note here that $\log(\frac{x}{y})=-\log(\frac{y}{x})$
\subsection{Odds ratios}
\begin{definition}[Odds Ratio]
	Comparing two different odds by dividing them
\end{definition}
This has the same problem that needs solving with logs as with a single odd\\
\\
Larger odds ratio mean that one thing is a good predictor of another.
\section{Logistic Regression}
This is similar to linear regression except it predicts true or false\\
\\
It follows an s shape, fitting to false for low numbers and true for high numbers, although can be adapted to be reversed.\\
\\
Logistic regression provides probabilities and classifies new samples using continuous and discrete measurements\\
\\
We use maximum likelihood to draw the regression line in the log graph
\newpage
$$\log(\frac{p}{1-p})=\log(odds)$$
$$\dfrac{p}{1-p}=e^{\log(odds)}$$
$$p=(1-p)e^{\log(odds)}=e^{\log(odds)}-p^{e\log(odds)}$$
$$p+pe^{\log(odds)}=e^{\log(odds)}$$
$$p(1+e^{\log(odds)})=e^{\log(odds)}$$
$$p=\dfrac{e^{\log(odds)}}{1+e^{\log(odds)}}$$
To then calculate the probability of success, multiply the probabilities of the points on the logistic regression line\\
\\
We can use these probabilities to calculate the likelihood, which can then be used to improve the fit of the line.






\end{document}
