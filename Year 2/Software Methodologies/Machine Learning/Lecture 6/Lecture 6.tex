\documentclass{article}[18pt]
\input{../../../../format}
\lhead{Software Methodologies - Machine Learning}


\begin{document}
\begin{center}
\underline{\huge Decision Trees and Random Forests}
\end{center}
\section{Decision Trees}
There might be multiple decision trees for deciding the same thing from different conditions. To decide which is best, we use Gini Impurity
$$\text{Gini Impurity}=1-(\text{the probability of Yes})^2-(\text{the Probability of No})^2$$
A weighted average should be used if the sample size is different\\
\\
The lower the value the better\\
\\
From a raw table of data to a decision tree:
\begin{enumerate}
	\item Calculate all of the Gini Impurity values
	\item If a node itself has the lowest value, leave it as a Leaf node, don't further separate it
	\item If separating the data results in an improvement, then pick the separation with the lowest Gini impurity value
\end{enumerate}
\subsection{Numeric Data}
To get impurities
\begin{enumerate}
	\item Sort the values lowest to highest
	\item Calculate the average for adjacent values
	\item Calculate the impurity values for each average weight
	\begin{itemize}
		\item For each average, look at the yes and no instances on the greater than and less than sections, use these for the probabilities
	\end{itemize}
\end{enumerate}



To Build a tree:
\begin{enumerate}
	\item Yes/no questions at each step
	\item Numeric data, like patient weight
\end{enumerate}
\subsection{Ranked Data and Multiple Choice Data}
Ranked Data
\begin{center}
	\includegraphics[scale=0.7]{"Ranked Data"}
\end{center}
Multiple Choices Data
\begin{center}
	\includegraphics[scale=0.7]{"Multiple Choices Data"}
\end{center}
\subsection{Missing data}
Options for boolean:
\begin{itemize}
	\item Choose the most common value in the column
	\item Find another column that has the highest correlation with the feature and use that as a guide
\end{itemize}
Options for numbers:
\begin{itemize}
	\item Use mean
	\item Use linear regression with another column with a good correlation
\end{itemize}
\section{Random Forests}
Why Random Forests:
\begin{itemize}
	\item Decision Trees are easy to build, use and interpret, but not flexible when classifying new samples
	\item Random forests combine the simplicity of decision trees with flexibility for better accuracy 
\end{itemize}
\subsection{How to build a random forest}
\textbf{Step 1} - Create a "bootstrapped" dataset:
\begin{itemize}
	\item Same size as the original dataset
	\item Randomly selected samples from the original dataset
	\item Samples can be selected more than once
\end{itemize}
\textbf{Step 2} - Build a decision tree using "bootstrapped" dataset, but only use a random subset of variables, e.g. 2\\
\\
\textbf{Step 3} - Go back to step 1 and repeat: make a new bootstrap  dataset and build a tree considering a subset of variables at each step (ideally 100's of times)
\begin{itemize}
	\item Using a bootstrapped sample and considering only a subset of the variables at each steps results in a wide variety of trees
	\item The variety makes random forests more effective than individual Decision Trees
\end{itemize}
\subsection{How to use a random forest}
\begin{itemize}
	\item Take the data and run it down the first tree we built
	\item Keep track of the result
	\item Then run the next data down the second tree
	\item Then run the next data down all the trees and what the majority of the trees choose is the outcome
\end{itemize}
\begin{definition}[Bagging]
	Bootstrapping the data plus using the aggregate to make a decision
\end{definition}
\subsection{Performance}
\begin{definition}[Out of bag dataset]
Data that was not used in the bootstrapped dataset
\end{definition}
\begin{itemize}
	\item Use the data that doesn't end up in the bootstrapped dataset for testing
	\item Run the data on the trees and see if the outcome is correctly predicted
	\item Use the number that correctly predict vs incorrectly predict as the measure
	\item Repeat for all samples and trees
\end{itemize}
\begin{definition}[Out of bag error]
	The proportion of out of bag samples that were incorrectly classified
\end{definition}

\end{document}
